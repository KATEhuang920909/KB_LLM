{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c050826-66cd-4eb4-9103-594d2291a856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from bert4torch.models import build_transformer_model\n",
    "from bert4torch.snippets import sequence_padding, text_segmentate\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from bert4torch.models import build_transformer_model, BaseModel\n",
    "from bert4torch.snippets import ListDataset\n",
    "from bert4torch.generation import SeqGeneration\n",
    "from bert4torch.callbacks import Callback, Logger\n",
    "from bert4torch.optimizers import get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import jieba \n",
    "from rouge_chinese import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig#, prepare_model_for_kbit_training  # 需要pip install git+https://github.com/huggingface/peft.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fca0adc-0682-43b9-a663-da29d8c076a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 基本参数\n",
    "mode = 'train'\n",
    "max_source_length = 1250\n",
    "max_target_length = 200\n",
    "lr = 5e-4\n",
    "batch_size = 16  # 根据显存大小调整\n",
    "eval_batch_size = 4\n",
    "grad_accumulation_steps = 1  # 根据显存大小调整\n",
    "max_seq_length = max_source_length + max_target_length\n",
    "ignore_pad_token_for_loss = True\n",
    "epochs = 1\n",
    "steps_per_epoch = 3000\n",
    "prefix = ''\n",
    "prompt_column = 'content'\n",
    "response_column = 'summary'\n",
    "history_column = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a14765-73b0-4df8-bf29-1b48ce7e3131",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "# 模型配置\n",
    "dir_path = \"/root/autodl-tmp/chatglm2-int4\"\n",
    "config_path = dir_path + '/bert4torch_config.json'\n",
    "# checkpoint_path = [dir_path + f'\\\\bert4torch_pytorch_model_{i}.bin' for i in range(1,8)]  # 可加载单个，也可以加载多个\n",
    "checkpoint_path = dir_path + '/bert4torch_pytorch_model.bin'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(dir_path, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c13fb80b-f6ac-4893-8d02-2fc55799c54d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "class MyDataset(ListDataset):\n",
    "    @staticmethod\n",
    "    def load_data(filename):\n",
    "        \"\"\"加载数据，并尽量分为不超过maxlen的句子\n",
    "        \"\"\"\n",
    "        D = []\n",
    "        with open(filename, encoding='utf-8') as f:\n",
    "            for l in f:\n",
    "                l = json.loads(l)\n",
    "                if l[\"task_type\"] ==\"table_extract\":\n",
    "                    prompt = generate_prompt_QA(\";\".join(l[\"instruction\"]),l[\"question\"])\n",
    "                elif l[\"task_type\"] ==\"tuple_extract\":\n",
    "                    prompt = generate_prompt_QuerySummary(\";\".join(l[\"instruction\"]),l[\"question\"])\n",
    "                answer=l[\"answer\"]\n",
    "                D.append((prompt, answer))\n",
    "                    \n",
    "        return D\n",
    "\n",
    "# 抽取表格信息\n",
    "def generate_prompt_QA(related_text, query: str, preprompt=\"\") -> str:\n",
    "    prompt_template = \"\"\"基于以下表格信息，来回答用户的问题。\n",
    "        如果无法从中得到答案，请说 \"没有找到该问题对应的知识\"，不允许在答案中添加编造成分，答案请使用中文。\n",
    "        已知内容:\n",
    "        {context}\n",
    "        问题:\n",
    "        '{question}'\"\"\"\n",
    "\n",
    "    prompt = preprompt + prompt_template.replace(\"{question}\", query).replace(\"{context}\", related_text)\n",
    "    return prompt\n",
    "\n",
    "def generate_prompt_QuerySummary(related_text, query: str, preprompt=\"\") -> str:\n",
    "    prompt_template = \"\"\"已知问题:\n",
    "        {question}，请提取问题的主宾二元组，格式：xx|||xx\"\"\"\n",
    "\n",
    "    prompt = preprompt + prompt_template.replace(\"{question}\", query).replace(\"{context}\", related_text)\n",
    "    return prompt\n",
    "\n",
    "def collate_train_fn(batch):\n",
    "    batch_token_ids, batch_labels = [], []\n",
    "    for prompt, answer in batch:\n",
    "        a_ids = tokenizer.encode(text=prompt, add_special_tokens=True, truncation=True, max_length=max_source_length)\n",
    "        b_ids = tokenizer.encode(text=answer, add_special_tokens=False, truncation=True, max_length=max_target_length)\n",
    "\n",
    "        context_length = len(a_ids)\n",
    "        input_ids = a_ids + b_ids + [tokenizer.eos_token_id]\n",
    "        labels = [tokenizer.pad_token_id] * context_length + b_ids + [tokenizer.eos_token_id]\n",
    "        batch_token_ids.append(input_ids)\n",
    "        batch_labels.append(labels)\n",
    "\n",
    "    batch_token_ids = torch.tensor(sequence_padding(batch_token_ids, value=tokenizer.pad_token_id), dtype=torch.long, device=device)\n",
    "    batch_labels = torch.tensor(sequence_padding(batch_labels, value=tokenizer.pad_token_id), dtype=torch.long, device=device)\n",
    "    return [batch_token_ids], batch_labels\n",
    "\n",
    "def collate_dev_fn(batch):\n",
    "    batch_prompt, batch_labels = [], []\n",
    "    for prompt, labels  in batch:\n",
    "        batch_prompt.append(prompt)\n",
    "        \n",
    "        label_ids = tokenizer(text_target=labels, max_length=max_target_length, truncation=True)['input_ids']\n",
    "        batch_labels.append(tokenizer.decode(label_ids, skip_special_tokens=True))\n",
    "    return batch_prompt, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ff58cf7-366e-41ac-9d45-59825ae8aa58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(MyDataset('../data/train_sft.json'), batch_size=batch_size, shuffle=True, collate_fn=collate_train_fn) \n",
    "dev_dataloader = DataLoader(MyDataset('../data/dev_sft.json'), batch_size=eval_batch_size, shuffle=False, collate_fn=collate_dev_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12dece59-1203-4cbf-9c81-1b8a950a4859",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Quantize linear layers: 100%|██████████| 168/168 [00:08<00:00, 19.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32m[INFO]\u001b[0m trainable params: 3784704 || all params: 3126754304 || trainable%: 0.12104257744710856\n"
     ]
    }
   ],
   "source": [
    "# 建立模型，加载权重\n",
    "model = build_transformer_model(config_path=config_path, checkpoint_path=checkpoint_path, model='glm2', add_trainer=True, \n",
    "                                tie_emb_prj_weight=True, # 绑定embedding和dense/lm_head的权重，transformers中有绑定\n",
    "                                ).half()\n",
    "\n",
    "# 量化\n",
    "load_in_nbit = None  # 设置为True在3060卡上loss能正常下降，在v100上loss就是nan\n",
    "if load_in_nbit == 8:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "\n",
    "    class CastOutputToFloat(nn.Sequential):\n",
    "        def forward(self, x):\n",
    "            return super().forward(x).to(torch.float32)\n",
    "    model = model.quantize(quantization_method='load_in_8bit', llm_int8_skip_modules=['model.embeddings.word_embeddings', 'lm_head']) # v3.0.0（含）之前lm_head换成dense\n",
    "    # model.dense = CastOutputToFloat(model.dense)  # v3.0.0（含）之前使用\n",
    "    model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "    \n",
    "elif load_in_nbit == 4:\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    q_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                bnb_4bit_quant_type='nf4',\n",
    "                                bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_compute_dtype=torch.float16,  # 可选 torch.float32, torch.float16, torch.bfloat16\n",
    "                                llm_int8_skip_modules=['model.embeddings.word_embeddings', 'lm_head']  # v3.0.0（含）之前lm_head换成dense\n",
    "                                )\n",
    "    model = model.quantize(quantization_method='load_in_4bit', quantization_config=q_config)\n",
    "    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "# lora\n",
    "peft_config = LoraConfig(\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=['q', 'k', 'v']\n",
    "    )\n",
    "model = model.get_peft_model(peft_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a43b182f-58af-421c-b0a9-ed7ab176a70b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraModel(\n",
       "  (model): BertBaseModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(65024, 4096, padding_idx=0)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (decoderLayer): ModuleList(\n",
       "      (0-27): 28 x GLMBlock(\n",
       "        (multiHeadAttention): MultiHeadAttentionLayer(\n",
       "          (q): Linear(\n",
       "            in_features=4096, out_features=4096, bias=True\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (k): Linear(\n",
       "            in_features=4096, out_features=256, bias=True\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (v): Linear(\n",
       "            in_features=4096, out_features=256, bias=True\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "          )\n",
       "          (o): QuantizedLinear(in_features=4096, out_features=4096, bias=False)\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (relative_positions_encoding): RoPEPositionEncoding()\n",
       "        )\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (layerNorm1): LayerNorm()\n",
       "        (feedForward): PositionWiseFeedForward(\n",
       "          (intermediateDense): QuantizedLinear(in_features=4096, out_features=27392, bias=False)\n",
       "          (outputDense): QuantizedLinear(in_features=13696, out_features=4096, bias=False)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "        (layerNorm2): LayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=4096, out_features=65024, bias=False)\n",
       "    (LayerNormFinal): LayerNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "198d9cf8-3be0-428d-9ad6-07efd292eb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(nn.CrossEntropyLoss):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def forward(self, logits, labels):\n",
    "        '''\n",
    "        logits: [btz, seq_len, vocab_size]\n",
    "        labels: token_ids: [btz, seq_len]\n",
    "        '''\n",
    "        raw_dtyps = logits.dtype\n",
    "        logits = logits.to(torch.float32)\n",
    "        print( logits.dtype,labels.dtype)\n",
    "        logits = logits[:, :-1, :].contiguous()  # 预测序列，错开一位\n",
    "        labels = labels[:, 1:].contiguous() # 目标token_ids\n",
    "        \n",
    "        logits = logits.reshape(-1, logits.shape[-1])\n",
    "        labels = labels.flatten()\n",
    "        loss = super().forward(logits, labels)\n",
    "\n",
    "        return loss.to(raw_dtyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "751b6c8b-52d6-4af5-8b9d-3e41637816fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, 0, steps_per_epoch*epochs)  # torch4keras<0.0.8需要设置为(steps_per_epoch*epochs)//grad_accumulation_steps\n",
    "model.compile(loss=CrossEntropyLoss(ignore_index=tokenizer.pad_token_id), optimizer=optimizer, scheduler=scheduler, grad_accumulation_steps=grad_accumulation_steps, clip_grad_norm=1.0)\n",
    "\n",
    "class Chat(SeqGeneration):\n",
    "    def pre_process(self, text):\n",
    "        return [tokenizer(text, max_length=max_source_length, truncation=True)['input_ids']]\n",
    "    def post_process(self, output_ids):\n",
    "        return [tokenizer.decode(output_id.cpu().numpy()) for output_id in output_ids]\n",
    "generation = Chat(model, tokenizer, start_id=None, end_id=tokenizer.eos_token_id, pad_id=tokenizer.pad_token_id, \n",
    "                  mode='random_sample', maxlen=512, default_rtype='logits', use_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0bef353-f159-44f8-9911-74e968768fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(data, epoch='final'):\n",
    "    preds, labels = [], []\n",
    "    for prompt, label in tqdm(data, desc='Evaluating'):\n",
    "        pred = generation.batch_generate(prompt, topk=50, topp=0.7, temperature=0.95)\n",
    "        preds.extend(pred)\n",
    "        labels.extend(label)\n",
    "        with open(f'./chatglm2_int4_lora_preds_{epoch}.txt', 'a+', encoding='utf-8') as f:\n",
    "            for pred_i, label_i in zip(pred, label):\n",
    "                f.write(json.dumps({'pred': pred_i, 'label': label_i}, ensure_ascii=False) + '\\n')\n",
    "    count,sums=0,0\n",
    "    for pred, label in zip(preds, labels):\n",
    "        sums+=1\n",
    "        if pred==label:\n",
    "            count+=1\n",
    "        \n",
    "    return {\"accuracy\":round(count/sums,5)}\n",
    "class Evaluator(Callback):\n",
    "    \"\"\"评估与保存\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.best = 0\n",
    "\n",
    "    def on_epoch_end(self, steps, epoch, logs=None):\n",
    "        score_dict=evaluate(dev_dataloader)\n",
    "        if score_dict[\"accuracy\"]>self.best:\n",
    "            self.best = score_dict[\"accuracy\"]\n",
    "            model.save_weights(f'chatglm2_int4_lora/model_{self.best}.pt', trainable_only=True)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b68c0529-b701-4a98-bf0a-9ddb0ed7f553",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-27 18:31:26 - Start Training\n",
      "\n",
      "2023-09-27 18:31:26 - Epoch: 1/1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "self and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m Evaluator()\n\u001b[1;32m      2\u001b[0m logger \u001b[38;5;241m=\u001b[39m Logger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./chatglm2_int4_lora.log\u001b[39m\u001b[38;5;124m'\u001b[39m, interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# score_dict = evaluator.evaluate(dev_dataloader)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch4keras/trainer.py:365\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_dataloader, steps_per_epoch, epochs, callbacks, verbose)\u001b[0m\n\u001b[1;32m    363\u001b[0m train_X, train_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_nextbatch()  \u001b[38;5;66;03m# 获取下一个batch的训练数据\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_first_step(resume_step, train_X, train_y)  \u001b[38;5;66;03m# log第一个step\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m output, loss, loss_detail \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_train_step_end()\n\u001b[1;32m    367\u001b[0m tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch4keras/trainer.py:178\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self, train_X, train_y)\u001b[0m\n\u001b[1;32m    176\u001b[0m         loss_detail \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(output, train_y)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     loss_detail \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(output, train_y)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# 整理loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch4keras/trainer.py:153\u001b[0m, in \u001b[0;36mTrainer._forward\u001b[0;34m(self, *inputs, **input_kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_kwargs):\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''调用模型的forward，方便下游继承的时候可以自定义使用哪个模型的forward\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_argparse_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch4keras/trainer.py:166\u001b[0m, in \u001b[0;36mTrainer._argparse_forward\u001b[0;34m(self, model, *inputs, **input_kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mforward(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_kwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mforward(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/bert4torch/models/base.py:125\u001b[0m, in \u001b[0;36mBERT_BASE.forward\u001b[0;34m(self, *inputs, **model_kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_embeddings(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Main\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_main_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Final\u001b[39;00m\n\u001b[1;32m    127\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_final_layers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/bert4torch/models/transformer.py:68\u001b[0m, in \u001b[0;36mDecoder.apply_main_layers\u001b[0;34m(self, **model_kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l_i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoderLayer):\n\u001b[1;32m     67\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_on_layer_begin(l_i, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m---> 68\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     model_kwargs\u001b[38;5;241m.\u001b[39mupdate(outputs)\n\u001b[1;32m     70\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m model_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/bert4torch/models/bert.py:95\u001b[0m, in \u001b[0;36mBERT.layer_forward\u001b[0;34m(self, layer, model_kwargs, use_reentrant)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m checkpoint(layer, use_reentrant\u001b[38;5;241m=\u001b[39muse_reentrant, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/bert4torch/layers/transformer_block.py:41\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, conditional_emb, encoder_hidden_states, encoder_attention_mask, past_key_value, cross_past_key_value, **model_kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, position_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, conditional_emb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, encoder_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m     38\u001b[0m             encoder_attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, past_key_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cross_past_key_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# ============== self attention ==============\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerNorm1(hidden_states, conditional_emb) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layernorm \u001b[38;5;28;01melse\u001b[39;00m hidden_states  \u001b[38;5;66;03m# pre/post layernorm\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     self_attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiHeadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# self.decoder为true时候，这里的attention_mask是三角的\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     residual \u001b[38;5;241m=\u001b[39m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_residual_post_layernorm \u001b[38;5;28;01melse\u001b[39;00m hidden_states\n\u001b[1;32m     43\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(self_attn_output[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/bert4torch/layers/attention.py:105\u001b[0m, in \u001b[0;36mMultiHeadAttentionLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, position_ids, **model_kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03mhidden_states shape: [batch_size, seq_q, hidden_size]\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03mattention_mask shape: [batch_size, 1, 1, seq_q] 或者 [batch_size, 1, seq_q, seq_q]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03mpast_key_value shape: ([batch_size, num_attention_heads, key_len_cache, attention_head_size], ...)\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# query_layer shape: [batch_size, num_attention_heads, query_len, attention_head_size]\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# key_layer shape: [batch_size, num_attention_heads, key_len, attention_head_size]\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# value_layer shape: [batch_size, num_attention_heads, value_len, attention_head_size]\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_q_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp_bias \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrotary\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# rotary有cache情况下，需要先rope后再和past_key_value concat\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_k_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk(hidden_states))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/peft/tuners/lora.py:565\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    563\u001b[0m     result \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlinear(x, transpose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfan_in_fan_out), bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerged:\n\u001b[0;32m--> 565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfan_in_fan_out\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter]\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    569\u001b[0m     result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_B[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter](\n\u001b[1;32m    571\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter](\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_dropout[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter](x))\n\u001b[1;32m    572\u001b[0m         )\n\u001b[1;32m    573\u001b[0m         \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter]\n\u001b[1;32m    574\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: self and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluator = Evaluator()\n",
    "logger = Logger('./chatglm2_int4_lora.log', interval=100)\n",
    "\n",
    "model.fit(train_dataloader, steps_per_epoch=steps_per_epoch, epochs=epochs, callbacks=[evaluator, logger])\n",
    "# score_dict = evaluator.evaluate(dev_dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8852b02-2908-410e-a0a3-5297b7ba4a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
